<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Licensed to the Apache Software Foundation (ASF) under one or more       -->
<!-- contributor license agreements.  See the NOTICE file distributed with    -->
<!-- this work for additional information regarding copyright ownership.      -->
<!-- The ASF licenses this file to You under the Apache License, Version 2.0  -->
<!-- (the "License"); you may not use this file except in compliance with     -->
<!-- the License.  You may obtain a copy of the License at                    -->
<!--                                                                          -->
<!--     http://www.apache.org/licenses/LICENSE-2.0                           -->
<!--                                                                          -->
<!-- Unless required by applicable law or agreed to in writing, software      -->
<!-- distributed under the License is distributed on an "AS IS" BASIS,        -->
<!-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. -->
<!-- See the License for the specific language governing permissions and      -->
<!-- limitations under the License.                                           -->

<configuration>

  <property>
    <!-- URI of NN. Fully qualified. No IP.-->
    <name>fs.defaultFS</name>
    <value>hdfs://<%= @nameservice_id %></value>
    <description>HA-enabled logical URI NameService ID</description>
  </property>

  <!-- HDFS HA automatic failover with Zookeeper -->
  <property>
    <name>ha.zookeeper.quorum</name>
    <value><%= @hadoop_namenode_nn1 %>:2181,<%= @hadoop_namenode_nn2 %>:2181,<%= @hadoop_resourcemanager %>:2181</value>
    <description>This lists the host-port pairs running the ZooKeeper service</description>
  </property>
 
  <property>
    <name>hadoop.security.authentication</name>
    <value><%= @hadoop_security_authentication %></value>
    <description>defines the authentication mechanism
    to use within Hadoop. By default, it is set to simple, which simply trusts the
    client is who they claim to be, whereas setting it to the string kerberos enables
    Kerberos support. Setting a value of simple here disable security</description>
  </property>


<% if @hadoop_security_authentication == "kerberos" %>

  <!--                               -->
  <!-- Hadoop Secure Mode - KERBEROS -->
  <!--                               -->

  <property>
    <name>hadoop.security.authorization</name>
    <value>true</value>
    <description>causes Hadoop to authorize the client when it makes remote procedure 
        calls to a server. The access control lists that affect these permissions are 
        configured via the hadoop-policy.xml file and allow per-service level control. 
        For instance, it is possible to permit only users placed in the
        mapred-admin Linux group to invoke APIs that are part of the administration service
        (the security.admin.operations.protocol.acl policy). When enabling security,
        this feature should be enabled as well and meaningful ACLs configured.</description>
  </property>

  <!-- Authentication for Hadoop HTTP web-consoles -->

  <property>
        <name>hadoop.http.filter.initializers</name>
        <value>org.apache.hadoop.security.AuthenticationFilterInitializer</value>
        <description>Similarly to Hadoop RPC, Hadoop HTTP web-consoles can be 
        configured to require Kerberos authentication using HTTP SPNEGO protocol.
        This property is the initializer class.
        </description>
  </property>  

  <property>
        <name>hadoop.http.authentication.type</name>
        <value>kerberos</value>
        <description>Defines authentication used for the HTTP web-consoles. 
        The supported values are: simple | kerberos | #AUTHENTICATION_HANDLER_CLASSNAME#
        </description>
  </property>  

  <property>
        <name>hadoop.http.authentication.kerberos.principal</name>
        <value>HTTP/_HOST@<%= @realm %></value>
        <description>Indicates the Kerberos principal to be used for 
        HTTP endpoint when using 'kerberos' authentication. The principal 
        short name must be HTTP per Kerberos HTTP SPNEGO specification. 
        The default value is HTTP/_HOST@$LOCALHOST, where _HOST -if present- 
        is replaced with bind address of the HTTP server.
        </description>
  </property> 

  <property>
        <name>hadoop.http.authentication.kerberos.keytab</name>
        <value>/etc/hadoop/conf/security/HTTP.keytab</value>
        <description> Location of the keytab file with the credentials 
        for the Kerberos principal used for the HTTP endpoint.
        </description>
  </property> 

  <property>
        <name>hadoop.http.authentication.simple.anonymous.allowed</name>
        <value>false</value>
        <description>Indicates if anonymous requests are allowed when 
        using 'simple' authentication. The default value is true.
        </description>
   </property> 

   <property>
        <name>hadoop.http.authentication.cookie.domain</name>
        <value><%= @realm.downcase %></value>
        <description>The domain to use for the HTTP cookie that stores 
        the authentication token. In order to authentiation to work correctly 
        across all nodes in the cluster the domain must be correctly set. 
        There is no default value, the HTTP cookie will not have a domain 
        working only with the hostname issuing the HTTP cookie.
        </description>
   </property> 

   <property>
        <name>hadoop.http.authentication.signature.secret.file</name>
        <value>/etc/hadoop/conf/security/secret</value>
        <description>The signature secret file for signing the authentication
        tokens. The same secret should be used for all nodes in the cluster.
        </description>
   </property> 

<!-- change only if default is no used
    <property>
        <name>hadoop.http.authentication.token.validity</name>
        <value>36000</value>
        <description>Indicates how long (in seconds) an authentication token 
        is valid before it has to be renewed. The default value is 36000.
        </description>
    </property> 
-->
<% end %> <!-- hadoop_security_authentication == "kerberos" -->

<% if has_variable?("hadoop_security_group_mapping") %>
  <property>
    <name>hadoop.security.group.mapping</name>
    <value><%= hadoop_security_group_mapping %></value>
  </property>
<% end %>

<% if has_variable?("hadoop_core_proxyusers") %>
<% hadoop_core_proxyusers.sort.each do |superuser, data| %>
  <property>
    <name>hadoop.proxyuser.<%= superuser %>.hosts</name>
    <value><%= data['hosts'] %></value>
  </property>
  <property>
    <name>hadoop.proxyuser.<%= superuser %>.groups</name>
    <value><%= data['groups'] %></value>
  </property>
<% end %>
<% end %>

<% if has_variable?("hadoop_snappy_codec") %>
  <property>
    <name>io.compression.codecs</name>
    <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec</value>
  </property>
  <!-- property>
    <name>io.compression.codec.lzo.class</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
  </property-->
<% end %>

<% if has_variable?("hadoop_config_fs_inmemory_size_mb") %>
  <property>
    <name>fs.inmemory.size.mb</name>
    <value><%= hadoop_config_fs_inmemory_size_mb %></value>
  </property>
<% end %>

<% if has_variable?("hadoop_config_io_file_buffer_size") %>
  <property>
    <name>io.file.buffer.size</name>
    <value><%= hadoop_config_io_file_buffer_size %></value>
  </property>
<% end %>

  <!--property>
  <name>ha.zookeeper.acl</name>
    <value>@/etc/hadoop/conf/security/zk-acl.txt</value>
  </property-->

 <!-- PROXYUSER -->

 <property>
        <name>hadoop.proxyuser.flume.groups</name>
        <value>*</value>
 </property>
 <property>
         <name>hadoop.proxyuser.flume.hosts</name>
         <value>*</value>
 </property>
 <property>
        <name>hadoop.proxyuser.oozie.groups</name>
        <value>*</value>
 </property>
 <property>
        <name>hadoop.proxyuser.oozie.hosts</name>
        <value>*</value>
 </property>

 <property>
        <name>hadoop.proxyuser.httpfs.hosts</name>
        <value>*</value>
 </property>
 <property>
        <name>hadoop.proxyuser.httpfs.groups</name>
        <value>*</value>
 </property>
 <property>
        <name>hadoop.proxyuser.hue.hosts</name>
        <value>*</value>
 </property>
 <property>
        <name>hadoop.proxyuser.hue.groups</name>
        <value>*</value>
 </property>
 <property>
        <name>hue.kerberos.principal.shortname</name>
        <value>hue</value>
 </property>

</configuration>

